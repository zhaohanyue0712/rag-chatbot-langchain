import streamlit as st
import os
import tempfile
import shutil
from typing import List, Dict, Any
import logging

# ---- LangChain / community / core ìµœì‹  ë²„ì „ í˜¸í™˜ ì„í¬íŠ¸ ----
from langchain_community.document_loaders import PyPDFLoader, TextLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import Chroma
from langchain_openai import ChatOpenAI
from langchain_core.prompts import PromptTemplate
from langchain.chains.retrieval_qa.base import RetrievalQA

# ---- ê¸°íƒ€ ë¼ì´ë¸ŒëŸ¬ë¦¬ ----
import pandas as pd
import plotly.express as px

# ---- ë¡œê¹… ì„¤ì • ----
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# ---------------------------
# í˜ì´ì§€ ì„¤ì •
# ---------------------------
st.set_page_config(
    page_title="ğŸ¤– ë‚˜ë§Œì˜ RAG ì±—ë´‡",
    page_icon="ğŸ¤–",
    layout="wide",
    initial_sidebar_state="expanded"
)

# ---------------------------
# ì»¤ìŠ¤í…€ CSS
# ---------------------------
st.markdown("""
<style>
    .main-header {
        font-size: 3.5rem;
        background: linear-gradient(90deg, #667eea 0%, #764ba2 100%);
        -webkit-background-clip: text;
        -webkit-text-fill-color: transparent;
        text-align: center;
        margin-bottom: 2rem;
        font-weight: bold;
    }
    .chat-container {
        background: #f5f7fa;
        border-radius: 15px;
        padding: 2rem;
        margin: 1rem 0;
    }
    .chat-message {
        padding: 1rem;
        border-radius: 12px;
        margin: 0.8rem 0;
    }
    .user-message {
        background: #667eea;
        color: white;
        margin-left: 20%;
    }
    .bot-message {
        background: #f093fb;
        color: white;
        margin-right: 20%;
    }
</style>
""", unsafe_allow_html=True)

# ---------------------------
# RAG ì±—ë´‡ í´ë˜ìŠ¤
# ---------------------------
class RAGChatbot:
    """RAG ì±—ë´‡ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.vectorstore = None
        self.qa_chain = None
        self.documents = []
        self.embeddings = None
        
    def initialize_embeddings(self):
        try:
            self.embeddings = HuggingFaceEmbeddings(
                model_name="sentence-transformers/all-MiniLM-L6-v2",
                model_kwargs={'device': 'cpu'}
            )
            return True
        except Exception as e:
            logger.error(f"ì„ë² ë”© ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return False
    
    def load_documents(self, uploaded_files: List) -> tuple:
        """ë¬¸ì„œ ë¡œë“œ ë° ì²˜ë¦¬ (Streamlit Cloud í™˜ê²½ í˜¸í™˜ ë²„ì „)"""
        try: 
            import pypdf  # PDF íŒŒì‹± ë°±ì—…
            temp_dir = tempfile.mkdtemp()
            all_documents = []

           # 1ï¸âƒ£ ì—…ë¡œë“œëœ íŒŒì¼ì„ ì„ì‹œ í´ë”ì— ì €ì¥
            for uploaded_file in uploaded_files:
                temp_file_path = os.path.join(temp_dir, uploaded_file.name)
                with open(temp_file_path, "wb") as f:
                  f.write(uploaded_file.getbuffer())

            # 2ï¸âƒ£ PDF / TXT ë¡œë” (pypdf í´ë°±)
             try:
                  if uploaded_file.name.lower().endswith(".pdf"):
                     loader = PyPDFLoader(temp_file_path)
                     docs = loader.load()
                  else:
                     loader = TextLoader(temp_file_path, encoding="utf-8")
                     docs = loader.load()
               except Exception:
                  reader = pypdf.PdfReader(temp_file_path)
                  text = "\n".join(page.extract_text() or "" for page in reader.pages)
                  docs = [Document(page_content=text, metadata={"source": uploaded_file.name})]

              all_documents.extend(docs)

             # 3ï¸âƒ£ í…ìŠ¤íŠ¸ ë¶„í• 
             if not all_documents:
                 raise RuntimeError("âŒ ì—…ë¡œë“œëœ ë¬¸ì„œì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")

            text_splitter = RecursiveCharacterTextSplitter(
                chunk_size=1000,
                chunk_overlap=200
            )
            texts = text_splitter.split_documents(all_documents)
            self.documents = texts

            # 4ï¸âƒ£ ì„ë² ë”© ì´ˆê¸°í™”
            if not self.initialize_embeddings():
              raise RuntimeError("âŒ ì„ë² ë”© ì´ˆê¸°í™” ì‹¤íŒ¨")

         # 5ï¸âƒ£ Chroma ì €ì¥ì†Œ ê²½ë¡œ ìƒì„± (/tmp ì‚¬ìš©)
          chroma_path = os.path.join(tempfile.gettempdir(), "chroma_db")
          if os.path.exists(chroma_path):
              shutil.rmtree(chroma_path)
          os.makedirs(chroma_path, exist_ok=True)

         # 6ï¸âƒ£ ë²¡í„° ìŠ¤í† ì–´ ìƒì„±
          self.vectorstore = Chroma.from_documents(
              documents=texts,
              embedding=self.embeddings,
              persist_directory=chroma_path
         )

         # 7ï¸âƒ£ ì„ì‹œ í´ë” ì •ë¦¬
          shutil.rmtree(temp_dir, ignore_errors=True)

          return self.vectorstore, len(texts)

       except Exception as e:
          logger.error(f"ë¬¸ì„œ ë¡œë”© ì‹¤íŒ¨: {e}")
          st.error(f"âŒ ë¬¸ì„œ ë¡œë”© ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")
          return None, 0

    def create_qa_chain(self, api_key: str) -> bool:
        try:
            prompt_template = """ë‹¤ìŒ ì»¨í…ìŠ¤íŠ¸ë¥¼ ì‚¬ìš©í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”. 
            ë‹µë³€ì€ í•œêµ­ì–´ë¡œ ì‘ì„±í•˜ê³ , ì œê³µëœ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì •í™•í•˜ê³  ë„ì›€ì´ ë˜ëŠ” ë‹µë³€ì„ í•´ì£¼ì„¸ìš”.
            ë§Œì•½ ì»¨í…ìŠ¤íŠ¸ì—ì„œ ë‹µì„ ì°¾ì„ ìˆ˜ ì—†ë‹¤ë©´, ê·¸ë ‡ê²Œ ë§ì”€í•´ì£¼ì„¸ìš”.
            ì»¨í…ìŠ¤íŠ¸: {context}
            ì§ˆë¬¸: {question}
            ë‹µë³€:"""
            PROMPT = PromptTemplate(
                template=prompt_template,
                input_variables=["context", "question"]
            )
            llm = ChatOpenAI(
                openai_api_key=api_key,
                model_name="gpt-3.5-turbo",
                temperature=0.7,
                max_tokens=1000
            )
            self.qa_chain = RetrievalQA.from_chain_type(
                llm=llm,
                chain_type="stuff",
                retriever=self.vectorstore.as_retriever(search_type="similarity", search_kwargs={"k": 4}),
                chain_type_kwargs={"prompt": PROMPT},
                return_source_documents=True
            )
            return True
        except Exception as e:
            logger.error(f"QA ì²´ì¸ ìƒì„± ì‹¤íŒ¨: {e}")
            return False
    
    def query(self, question: str) -> Dict[str, Any]:
        try:
            if not self.qa_chain:
                return {"error": "QA ì²´ì¸ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."}
            response = self.qa_chain({"query": question})
            return {
                "answer": response["result"],
                "source_documents": response.get("source_documents", [])
            }
        except Exception as e:
            return {"error": f"ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜: {str(e)}"}

# ---------------------------
# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
# ---------------------------
def initialize_session_state():
    if 'messages' not in st.session_state:
        st.session_state.messages = []
    if 'chatbot' not in st.session_state:
        st.session_state.chatbot = RAGChatbot()
    if 'documents_loaded' not in st.session_state:
        st.session_state.documents_loaded = False
    if 'qa_chain_ready' not in st.session_state:
        st.session_state.qa_chain_ready = False

# ---------------------------
# ë©”ì‹œì§€ í‘œì‹œ í•¨ìˆ˜
# ---------------------------
def display_chat_message(role: str, content: str):
    if role == "user":
        st.markdown(f"<div class='chat-message user-message'>{content}</div>", unsafe_allow_html=True)
    else:
        st.markdown(f"<div class='chat-message bot-message'>{content}</div>", unsafe_allow_html=True)

# ---------------------------
# ë©”ì¸ í•¨ìˆ˜
# ---------------------------
def main():
    initialize_session_state()

    st.markdown('<h1 class="main-header">ğŸ¤– ë‚˜ë§Œì˜ RAG ì±—ë´‡</h1>', unsafe_allow_html=True)

    with st.sidebar:
        st.markdown("## âš™ï¸ ì„¤ì •")
        api_key = st.text_input("ğŸ”‘ OpenAI API í‚¤", type="password")
        if api_key:
            st.session_state.openai_api_key = api_key
        st.markdown("## ğŸ“„ ë¬¸ì„œ ì—…ë¡œë“œ")
        uploaded_files = st.file_uploader("PDF ë˜ëŠ” TXT íŒŒì¼ ì—…ë¡œë“œ", type=['pdf', 'txt'], accept_multiple_files=True)
        if uploaded_files and st.button("ğŸ“š ë¬¸ì„œ ë¡œë“œ"):
            with st.spinner("ë¬¸ì„œë¥¼ ì²˜ë¦¬ ì¤‘..."):
                vectorstore, doc_count = st.session_state.chatbot.load_documents(uploaded_files)
                if vectorstore:
                    st.session_state.chatbot.vectorstore = vectorstore
                    st.session_state.documents_loaded = True
                    if api_key and st.session_state.chatbot.create_qa_chain(api_key):
                        st.session_state.qa_chain_ready = True
                        st.success(f"âœ… {doc_count}ê°œ ì²­í¬ ë¡œë“œ ì™„ë£Œ!")
                    else:
                        st.error("âŒ API í‚¤ ì˜¤ë¥˜")
                else:
                    st.error("âŒ ë¬¸ì„œ ë¡œë”© ì‹¤íŒ¨")

    # ë©”ì¸ ì±„íŒ… UI
    col1, col2 = st.columns([4, 1])
    with col1:
        st.markdown('<div class="chat-container">', unsafe_allow_html=True)
        for message in st.session_state.messages:
            display_chat_message(message["role"], message["content"])
        st.markdown('</div>', unsafe_allow_html=True)

    # ì…ë ¥ì°½ (container ë°–)
    prompt = st.chat_input("ğŸ’¬ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”...")

    if prompt:
        st.session_state.messages.append({"role": "user", "content": prompt})
        display_chat_message("user", prompt)
        if st.session_state.qa_chain_ready:
            with st.spinner("ğŸ¤” ë‹µë³€ ìƒì„± ì¤‘..."):
                response = st.session_state.chatbot.query(prompt)
                if "error" in response:
                    answer = response["error"]
                else:
                    answer = response["answer"]
                    sources = response.get("source_documents", [])
                    if sources:
                        answer += "\n\nğŸ“š **ì°¸ì¡° ë¬¸ì„œ:**"
                        for i, doc in enumerate(sources[:3], 1):
                            source_name = doc.metadata.get('source', 'Unknown')
                            answer += f"\n{i}. {os.path.basename(source_name)}"
                st.session_state.messages.append({"role": "assistant", "content": answer})
                display_chat_message("assistant", answer)
        else:
            msg = "âš ï¸ ë¨¼ì € ë¬¸ì„œë¥¼ ì—…ë¡œë“œí•˜ê³  API í‚¤ë¥¼ ì„¤ì •í•˜ì„¸ìš”."
            st.session_state.messages.append({"role": "assistant", "content": msg})
            display_chat_message("assistant", msg)

    with col2:
        st.markdown("## âš™ï¸ ì¶”ê°€ ê¸°ëŠ¥")
        if st.button("ğŸ—‘ï¸ ëŒ€í™” ì´ˆê¸°í™”"):
            st.session_state.messages = []
            st.rerun()

# ---------------------------
# ì‹¤í–‰
# ---------------------------
if __name__ == "__main__":
    main()
