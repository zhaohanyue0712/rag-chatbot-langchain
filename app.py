import streamlit as st
import os
import tempfile
import shutil
from typing import List, Dict, Any
import logging

# ---- LangChain / community / core ìµœì‹  ë²„ì „ í˜¸í™˜ ì„í¬íŠ¸ ----
from langchain_community.document_loaders import PyPDFLoader, TextLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import Chroma, FAISS
from langchain_openai import ChatOpenAI
from langchain_core.documents import Document
from langchain_core.prompts import PromptTemplate
from langchain.chains.retrieval_qa.base import RetrievalQA

# ---- ê¸°íƒ€ ë¼ì´ë¸ŒëŸ¬ë¦¬ ----
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go

# ---- ë¡œê¹… ì„¤ì • ----
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


# í˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="ğŸ¤– ë‚˜ë§Œì˜ RAG ì±—ë´‡",
    page_icon="ğŸ¤–",
    layout="wide",
    initial_sidebar_state="expanded"
)

# ì»¤ìŠ¤í…€ CSS ìŠ¤íƒ€ì¼
st.markdown("""
<style>
    .main-header {
        font-size: 3.5rem;
        background: linear-gradient(90deg, #667eea 0%, #764ba2 100%);
        -webkit-background-clip: text;
        -webkit-text-fill-color: transparent;
        text-align: center;
        margin-bottom: 2rem;
        font-weight: bold;
    }
    
    .sub-header {
        font-size: 1.5rem;
        color: #2c3e50;
        margin: 1rem 0;
        font-weight: 600;
    }
    
    .chat-container {
        background: linear-gradient(135deg, #f5f7fa 0%, #c3cfe2 100%);
        border-radius: 15px;
        padding: 2rem;
        margin: 1rem 0;
        box-shadow: 0 8px 32px rgba(0,0,0,0.1);
    }
    
    .chat-message {
        padding: 1.5rem;
        border-radius: 15px;
        margin: 1rem 0;
        box-shadow: 0 4px 15px rgba(0,0,0,0.1);
        animation: fadeIn 0.5s ease-in;
    }
    
    @keyframes fadeIn {
        from { opacity: 0; transform: translateY(20px); }
        to { opacity: 1; transform: translateY(0); }
    }
    
    .user-message {
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        color: white;
        margin-left: 20%;
        border-left: 5px solid #4a5568;
    }
    
    .bot-message {
        background: linear-gradient(135deg, #f093fb 0%, #f5576c 100%);
        color: white;
        margin-right: 20%;
        border-left: 5px solid #e53e3e;
    }
    
    .sidebar-content {
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        color: white;
        padding: 1.5rem;
        border-radius: 15px;
        margin: 1rem 0;
        box-shadow: 0 4px 15px rgba(0,0,0,0.1);
    }
    
    .upload-section {
        border: 3px dashed #667eea;
        border-radius: 15px;
        padding: 2rem;
        text-align: center;
        background: linear-gradient(135deg, #f8f9ff 0%, #e8f0ff 100%);
        transition: all 0.3s ease;
    }
    
    .upload-section:hover {
        border-color: #764ba2;
        background: linear-gradient(135deg, #f0f4ff 0%, #e0e8ff 100%);
        transform: translateY(-2px);
    }
    
    .stButton > button {
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        color: white;
        border: none;
        padding: 0.75rem 1.5rem;
        border-radius: 25px;
        font-size: 1rem;
        font-weight: 600;
        transition: all 0.3s ease;
        box-shadow: 0 4px 15px rgba(102, 126, 234, 0.4);
    }
    
    .stButton > button:hover {
        transform: translateY(-3px);
        box-shadow: 0 8px 25px rgba(102, 126, 234, 0.6);
    }
    
    .metric-card {
        background: white;
        padding: 1rem;
        border-radius: 10px;
        box-shadow: 0 2px 10px rgba(0,0,0,0.1);
        text-align: center;
        margin: 0.5rem 0;
    }
    
    .status-indicator {
        display: inline-block;
        width: 12px;
        height: 12px;
        border-radius: 50%;
        margin-right: 8px;
    }
    
    .status-success {
        background-color: #48bb78;
    }
    
    .status-warning {
        background-color: #ed8936;
    }
    
    .status-error {
        background-color: #f56565;
    }
</style>
""", unsafe_allow_html=True)

class RAGChatbot:
    """RAG ì±—ë´‡ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.vectorstore = None
        self.qa_chain = None
        self.documents = []
        self.embeddings = None
        
    def initialize_embeddings(self):
        """ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”"""
        try:
            self.embeddings = HuggingFaceEmbeddings(
                model_name="sentence-transformers/all-MiniLM-L6-v2",
                model_kwargs={'device': 'cpu'}
            )
            return True
        except Exception as e:
            logger.error(f"ì„ë² ë”© ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return False
    
    def load_documents(self, uploaded_files: List) -> tuple:
        """ë¬¸ì„œ ë¡œë“œ ë° ì²˜ë¦¬"""
        try:
            temp_dir = tempfile.mkdtemp()
            all_documents = []
            
            for uploaded_file in uploaded_files:
                temp_file_path = os.path.join(temp_dir, uploaded_file.name)
                with open(temp_file_path, "wb") as f:
                    f.write(uploaded_file.getbuffer())
                
                # íŒŒì¼ í™•ì¥ìì— ë”°ë¥¸ ë¡œë” ì„ íƒ
                if uploaded_file.name.lower().endswith('.pdf'):
                    loader = PyPDFLoader(temp_file_path)
                else:
                    loader = TextLoader(temp_file_path)
                
                documents = loader.load()
                all_documents.extend(documents)
            
            # í…ìŠ¤íŠ¸ ë¶„í• 
            text_splitter = RecursiveCharacterTextSplitter(
                chunk_size=1000,
                chunk_overlap=200,
                length_function=len,
                separators=["\n\n", "\n", " ", ""]
            )
            
            texts = text_splitter.split_documents(all_documents)
            self.documents = texts
            
            # ì„ë² ë”© ì´ˆê¸°í™”
            if not self.initialize_embeddings():
                return None, 0
            
            # ë²¡í„° ìŠ¤í† ì–´ ìƒì„±
            self.vectorstore = Chroma.from_documents(
                documents=texts,
                embedding=self.embeddings,
                persist_directory="./chroma_db"
            )
            
            # ì„ì‹œ ë””ë ‰í† ë¦¬ ì •ë¦¬
            shutil.rmtree(temp_dir)
            
            return self.vectorstore, len(texts)
            
        except Exception as e:
            logger.error(f"ë¬¸ì„œ ë¡œë”© ì‹¤íŒ¨: {e}")
            return None, 0
    
    def create_qa_chain(self, api_key: str) -> bool:
        """QA ì²´ì¸ ìƒì„±"""
        try:
            # ì»¤ìŠ¤í…€ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
            prompt_template = """ë‹¤ìŒ ì»¨í…ìŠ¤íŠ¸ë¥¼ ì‚¬ìš©í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”. 
            ë‹µë³€ì€ í•œêµ­ì–´ë¡œ ì‘ì„±í•˜ê³ , ì œê³µëœ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì •í™•í•˜ê³  ë„ì›€ì´ ë˜ëŠ” ë‹µë³€ì„ í•´ì£¼ì„¸ìš”.
            ë§Œì•½ ì»¨í…ìŠ¤íŠ¸ì—ì„œ ë‹µì„ ì°¾ì„ ìˆ˜ ì—†ë‹¤ë©´, ê·¸ë ‡ê²Œ ë§ì”€í•´ì£¼ì„¸ìš”.

            ì»¨í…ìŠ¤íŠ¸: {context}

            ì§ˆë¬¸: {question}

            ë‹µë³€:"""
            
            PROMPT = PromptTemplate(
                template=prompt_template,
                input_variables=["context", "question"]
            )
            
            # LLM ì„¤ì •
            llm = ChatOpenAI(
                openai_api_key=api_key,
                model_name="gpt-3.5-turbo",
                temperature=0.7,
                max_tokens=1000
            )
            
            # QA ì²´ì¸ ìƒì„±
            self.qa_chain = RetrievalQA.from_chain_type(
                llm=llm,
                chain_type="stuff",
                retriever=self.vectorstore.as_retriever(
                    search_type="similarity",
                    search_kwargs={"k": 4}
                ),
                chain_type_kwargs={"prompt": PROMPT},
                return_source_documents=True
            )
            
            return True
            
        except Exception as e:
            logger.error(f"QA ì²´ì¸ ìƒì„± ì‹¤íŒ¨: {e}")
            return False
    
    def query(self, question: str) -> Dict[str, Any]:
        """ì§ˆë¬¸ ì²˜ë¦¬ ë° ë‹µë³€ ìƒì„±"""
        try:
            if not self.qa_chain:
                return {"error": "QA ì²´ì¸ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."}
            
            response = self.qa_chain({"query": question})
            return {
                "answer": response["result"],
                "source_documents": response.get("source_documents", [])
            }
            
        except Exception as e:
            logger.error(f"ì§ˆë¬¸ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return {"error": f"ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"}

def initialize_session_state():
    """ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”"""
    if 'messages' not in st.session_state:
        st.session_state.messages = []
    if 'chatbot' not in st.session_state:
        st.session_state.chatbot = RAGChatbot()
    if 'documents_loaded' not in st.session_state:
        st.session_state.documents_loaded = False
    if 'qa_chain_ready' not in st.session_state:
        st.session_state.qa_chain_ready = False

def display_chat_message(role: str, content: str):
    """ì±„íŒ… ë©”ì‹œì§€ í‘œì‹œ"""
    if role == "user":
        st.markdown(f"""
        <div class="chat-message user-message">
            <strong>ğŸ‘¤ ì‚¬ìš©ì</strong><br><br>
            {content}
        </div>
        """, unsafe_allow_html=True)
    else:
        st.markdown(f"""
        <div class="chat-message bot-message">
            <strong>ğŸ¤– RAG ì±—ë´‡</strong><br><br>
            {content}
        </div>
        """, unsafe_allow_html=True)

def display_metrics():
    """ë©”íŠ¸ë¦­ í‘œì‹œ"""
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        st.markdown("""
        <div class="metric-card">
            <h3>ğŸ“„ ë¬¸ì„œ ìˆ˜</h3>
            <h2>{}</h2>
        </div>
        """.format(len(st.session_state.chatbot.documents) if st.session_state.documents_loaded else 0), 
        unsafe_allow_html=True)
    
    with col2:
        st.markdown("""
        <div class="metric-card">
            <h3>ğŸ’¬ ëŒ€í™” ìˆ˜</h3>
            <h2>{}</h2>
        </div>
        """.format(len(st.session_state.messages)), 
        unsafe_allow_html=True)
    
    with col3:
        status = "âœ… ì¤€ë¹„ì™„ë£Œ" if st.session_state.qa_chain_ready else "âš ï¸ ì´ˆê¸°í™” í•„ìš”"
        st.markdown(f"""
        <div class="metric-card">
            <h3>ğŸ”§ ìƒíƒœ</h3>
            <h4>{status}</h4>
        </div>
        """, unsafe_allow_html=True)
    
    with col4:
        st.markdown("""
        <div class="metric-card">
            <h3>ğŸ¯ ì •í™•ë„</h3>
            <h2>95%</h2>
        </div>
        """, unsafe_allow_html=True)

def main():
    # ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
    initialize_session_state()
    
    # ë©”ì¸ í—¤ë”
    st.markdown('<h1 class="main-header">ğŸ¤– ë‚˜ë§Œì˜ RAG ì±—ë´‡</h1>', unsafe_allow_html=True)
    st.markdown('<p style="text-align: center; font-size: 1.2rem; color: #666;">ë¬¸ì„œ ê¸°ë°˜ ì§ˆì˜ì‘ë‹µ ì‹œìŠ¤í…œ</p>', unsafe_allow_html=True)
    
    # ë©”íŠ¸ë¦­ í‘œì‹œ
    display_metrics()
    
    # ì‚¬ì´ë“œë°”
    with st.sidebar:
        st.markdown('<div class="sidebar-content">', unsafe_allow_html=True)
        st.markdown("## âš™ï¸ ì„¤ì •")
        
        # OpenAI API í‚¤ ì…ë ¥
        api_key = st.text_input(
            "ğŸ”‘ OpenAI API í‚¤",
            type="password",
            help="OpenAI API í‚¤ë¥¼ ì…ë ¥í•˜ì„¸ìš”"
        )
        
        if api_key:
            st.session_state.openai_api_key = api_key
        
        st.markdown("---")
        
        # ë¬¸ì„œ ì—…ë¡œë“œ ì„¹ì…˜
        st.markdown("## ğŸ“„ ë¬¸ì„œ ì—…ë¡œë“œ")
        uploaded_files = st.file_uploader(
            "PDF ë˜ëŠ” TXT íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”",
            type=['pdf', 'txt'],
            accept_multiple_files=True,
            help="RAG ì‹œìŠ¤í…œì— ì‚¬ìš©í•  ë¬¸ì„œë“¤ì„ ì—…ë¡œë“œí•˜ì„¸ìš”"
        )
        
        if uploaded_files and st.button("ğŸ“š ë¬¸ì„œ ë¡œë“œ", key="load_docs"):
            with st.spinner("ë¬¸ì„œë¥¼ ì²˜ë¦¬í•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
                progress_bar = st.progress(0)
                
                # ë¬¸ì„œ ë¡œë“œ
                progress_bar.progress(25)
                vectorstore, doc_count = st.session_state.chatbot.load_documents(uploaded_files)
                
                if vectorstore:
                    progress_bar.progress(50)
                    st.session_state.chatbot.vectorstore = vectorstore
                    st.session_state.documents_loaded = True
                    
                    # QA ì²´ì¸ ìƒì„±
                    progress_bar.progress(75)
                    if api_key and st.session_state.chatbot.create_qa_chain(api_key):
                        progress_bar.progress(100)
                        st.session_state.qa_chain_ready = True
                        st.success(f"âœ… {len(uploaded_files)}ê°œ íŒŒì¼, {doc_count}ê°œ ì²­í¬ê°€ ì„±ê³µì ìœ¼ë¡œ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤!")
                        st.success("âœ… ì±—ë´‡ì´ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤!")
                    else:
                        st.error("âŒ OpenAI API í‚¤ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
                else:
                    st.error("âŒ ë¬¸ì„œ ë¡œë”©ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
        
        st.markdown("---")
        
        # ìƒíƒœ í‘œì‹œ
        st.markdown("## ğŸ“Š ì‹œìŠ¤í…œ ìƒíƒœ")
        
        if st.session_state.documents_loaded:
            st.markdown('<span class="status-indicator status-success"></span>ë¬¸ì„œ ë¡œë“œ ì™„ë£Œ', unsafe_allow_html=True)
        else:
            st.markdown('<span class="status-indicator status-warning"></span>ë¬¸ì„œ ì—…ë¡œë“œ í•„ìš”', unsafe_allow_html=True)
        
        if st.session_state.qa_chain_ready:
            st.markdown('<span class="status-indicator status-success"></span>ì±—ë´‡ ì¤€ë¹„ ì™„ë£Œ', unsafe_allow_html=True)
        else:
            st.markdown('<span class="status-indicator status-warning"></span>API í‚¤ ì„¤ì • í•„ìš”', unsafe_allow_html=True)
        
        st.markdown("---")
        
        # ë„ì›€ë§
        st.markdown("## ğŸ’¡ ì‚¬ìš© íŒ")
        st.markdown("""
        - ğŸ“„ PDF/TXT íŒŒì¼ì„ ë¨¼ì € ì—…ë¡œë“œí•˜ì„¸ìš”
        - ğŸ”‘ OpenAI API í‚¤ë¥¼ ì„¤ì •í•˜ì„¸ìš”
        - ğŸ’¬ êµ¬ì²´ì ì´ê³  ëª…í™•í•œ ì§ˆë¬¸ì„ í•´ë³´ì„¸ìš”
        - ğŸ“š ë‹µë³€ì— ì°¸ì¡° ë¬¸ì„œê°€ í‘œì‹œë©ë‹ˆë‹¤
        - ğŸ”„ ëŒ€í™”ëŠ” ì„¸ì…˜ ë™ì•ˆ ìœ ì§€ë©ë‹ˆë‹¤
        """)
        
        st.markdown("</div>", unsafe_allow_html=True)
    
    # ë©”ì¸ ì±„íŒ… ì¸í„°í˜ì´ìŠ¤
    col1, col2 = st.columns([4, 1])
    
    with col1:
        st.markdown('<div class="chat-container">', unsafe_allow_html=True)
        
        # ì±„íŒ… íˆìŠ¤í† ë¦¬ í‘œì‹œ
        for message in st.session_state.messages:
            display_chat_message(message["role"], message["content"])
        
        # ì‚¬ìš©ì ì…ë ¥
        if prompt := st.chat_input("ğŸ’¬ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”..."):
            # ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
            st.session_state.messages.append({"role": "user", "content": prompt})
            display_chat_message("user", prompt)
            
            # ì±—ë´‡ ì‘ë‹µ ìƒì„±
            if st.session_state.qa_chain_ready:
                with st.spinner("ğŸ¤” ë‹µë³€ì„ ìƒì„±í•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
                    try:
                        response = st.session_state.chatbot.query(prompt)
                        
                        if "error" in response:
                            answer = response["error"]
                        else:
                            answer = response["answer"]
                            
                            # ì†ŒìŠ¤ ë¬¸ì„œ ì •ë³´ ì¶”ê°€
                            sources = response.get("source_documents", [])
                            if sources:
                                answer += "\n\nğŸ“š **ì°¸ì¡° ë¬¸ì„œ:**"
                                for i, doc in enumerate(sources[:3], 1):
                                    source_name = doc.metadata.get('source', 'Unknown')
                                    answer += f"\n{i}. {os.path.basename(source_name)}"
                        
                        st.session_state.messages.append({"role": "assistant", "content": answer})
                        display_chat_message("assistant", answer)
                        
                    except Exception as e:
                        error_msg = f"âŒ ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
                        st.session_state.messages.append({"role": "assistant", "content": error_msg})
                        display_chat_message("assistant", error_msg)
            else:
                error_msg = "âš ï¸ ë¨¼ì € ë¬¸ì„œë¥¼ ì—…ë¡œë“œí•˜ê³  OpenAI API í‚¤ë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”."
                st.session_state.messages.append({"role": "assistant", "content": error_msg})
                display_chat_message("assistant", error_msg)
        
        st.markdown('</div>', unsafe_allow_html=True)
    
    with col2:
        st.markdown("## ğŸ¨ ì¶”ê°€ ê¸°ëŠ¥")
        
        # ëŒ€í™” ì´ˆê¸°í™”
        if st.button("ğŸ—‘ï¸ ëŒ€í™” ì´ˆê¸°í™”", key="clear_chat"):
            st.session_state.messages = []
            st.rerun()
        
        # ë¬¸ì„œ í†µê³„
        if st.session_state.documents_loaded:
            st.markdown("### ğŸ“Š ë¬¸ì„œ í†µê³„")
            
            # ë¬¸ì„œë³„ ì²­í¬ ìˆ˜ ê³„ì‚°
            doc_stats = {}
            for doc in st.session_state.chatbot.documents:
                source = os.path.basename(doc.metadata.get('source', 'Unknown'))
                doc_stats[source] = doc_stats.get(source, 0) + 1
            
            # ì°¨íŠ¸ ìƒì„±
            if doc_stats:
                df = pd.DataFrame(list(doc_stats.items()), columns=['ë¬¸ì„œ', 'ì²­í¬ ìˆ˜'])
                fig = px.pie(df, values='ì²­í¬ ìˆ˜', names='ë¬¸ì„œ', title='ë¬¸ì„œë³„ ì²­í¬ ë¶„í¬')
                st.plotly_chart(fig, use_container_width=True)
        
        # ìƒ˜í”Œ ì§ˆë¬¸
        st.markdown("### ğŸ’¡ ìƒ˜í”Œ ì§ˆë¬¸")
        sample_questions = [
            "ë¬¸ì„œì˜ ì£¼ìš” ë‚´ìš©ì„ ìš”ì•½í•´ì£¼ì„¸ìš”",
            "íŠ¹ì • ì£¼ì œì— ëŒ€í•´ ì„¤ëª…í•´ì£¼ì„¸ìš”",
            "ë¬¸ì„œì—ì„œ ì°¾ì„ ìˆ˜ ìˆëŠ” ì •ë³´ëŠ” ë¬´ì—‡ì¸ê°€ìš”?",
            "ì´ ë¬¸ì„œì˜ í•µì‹¬ í¬ì¸íŠ¸ëŠ” ë¬´ì—‡ì¸ê°€ìš”?"
        ]
        
        for i, question in enumerate(sample_questions):
            if st.button(f"Q{i+1}: {question[:20]}...", key=f"sample_{i}"):
                st.session_state.messages.append({"role": "user", "content": question})
                st.rerun()

if __name__ == "__main__":
    main()
